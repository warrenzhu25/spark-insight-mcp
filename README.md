# MCP Server for Apache Spark History Server

[![CI](https://github.com/DeepDiagnostix-AI/mcp-apache-spark-history-server/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/DeepDiagnostix-AI/mcp-apache-spark-history-server/actions)
[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![MCP](https://img.shields.io/badge/MCP-Compatible-green.svg)](https://modelcontextprotocol.io/)
[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

> **ğŸ¤– Connect AI agents to Apache Spark History Server for intelligent job analysis and performance monitoring**

Transform your Spark infrastructure monitoring with AI! This Model Context Protocol (MCP) server enables AI agents to analyze job performance, identify bottlenecks, and provide intelligent insights from your Spark History Server data.

**âœ¨ Now featuring SparkInsight-inspired intelligent analysis tools** for auto-scaling optimization, data skew detection, failure analysis, and comprehensive performance insights!

## ğŸ¯ What is This?

**Spark History Server MCP** bridges AI agents with your existing Apache Spark infrastructure, enabling:

- ğŸ” **Query job details** through natural language
- ğŸ“Š **Analyze performance metrics** across applications
- ğŸ”„ **Compare multiple jobs** to identify regressions
- ğŸš¨ **Investigate failures** with detailed error analysis
- ğŸ“ˆ **Generate insights** from historical execution data

ğŸ“º **See it in action:**

[![Watch the demo video](https://img.shields.io/badge/YouTube-Watch%20Demo-red?style=for-the-badge&logo=youtube)](https://www.youtube.com/watch?v=e3P_2_RiUHw)


## ğŸ—ï¸ Architecture

```mermaid
graph TB
    A[ğŸ¤– AI Agent/LLM] --> F[ğŸ“¡ MCP Client]
    B[ğŸ¦™ LlamaIndex Agent] --> F
    C[ğŸŒ LangGraph] --> F
    D[ï¿½ï¸ Claudep Desktop] --> F
    E[ğŸ› ï¸ Amazon Q CLI] --> F

    F --> G[âš¡ Spark History MCP Server]

    G --> H[ğŸ”¥ Prod Spark History Server]
    G --> I[ğŸ”¥ Staging Spark History Server]
    G --> J[ğŸ”¥ Dev Spark History Server]

    H --> K[ğŸ“„ Prod Event Logs]
    I --> L[ğŸ“„ Staging Event Logs]
    J --> M[ğŸ“„ Dev Event Logs]
```

**ğŸ”— Components:**
- **ğŸ”¥ Spark History Server**: Your existing infrastructure serving Spark event data
- **âš¡ MCP Server**: This project - provides MCP tools for querying Spark data
- **ğŸ¤– AI Agents**: LangChain, custom agents, or any MCP-compatible client

## âš¡ Quick Start

The server supports **dual-mode operation**:
- ğŸ¤– **MCP Server Mode** (default) - For AI agents and LLM integration
- ğŸ–¥ï¸ **CLI Mode** (new!) - Direct command-line interface for human users

### ğŸ“‹ Prerequisites
- ğŸ”¥ Existing Spark History Server (running and accessible)
- ğŸ Python 3.12+
- âš¡ [uv](https://docs.astral.sh/uv/getting-started/installation/) package manager

### ğŸš€ Setup & Testing

```bash
git clone https://github.com/DeepDiagnostix-AI/mcp-apache-spark-history-server.git
cd mcp-apache-spark-history-server

# Install Task (if not already installed)
brew install go-task  # macOS, see https://taskfile.dev/installation/ for others

# Setup and start testing
task start-spark-bg            # Start Spark History Server with sample data (default Spark 3.5.5)
# Or specify a different Spark version:
# task start-spark-bg spark_version=3.5.2
task start-mcp-bg             # Start MCP Server

# Optional: Opens MCP Inspector on http://localhost:6274 for interactive testing
# Requires Node.js: 22.7.5+ (Check https://github.com/modelcontextprotocol/inspector for latest requirements)
task start-inspector-bg       # Start MCP Inspector

# When done, run `task stop-all`
```

## ğŸ–¥ï¸ CLI Mode (New!)

**Direct command-line interface** for human users - no MCP client required!

### ğŸš€ Quick CLI Setup
```bash
# Clone and install
git clone https://github.com/DeepDiagnostix-AI/mcp-apache-spark-history-server.git
cd mcp-apache-spark-history-server
uv sync

# Try CLI commands immediately
uv run spark-mcp --cli apps list --limit 5
uv run spark-mcp --cli --help
```

### ğŸ¯ CLI Examples
```bash
# List applications with beautiful formatting
uv run spark-mcp --cli apps list --limit 10 --format table

# Get comprehensive insights
uv run spark-mcp --cli analyze insights app-20231201-123456

# Compare applications (new stateful approach)
uv run spark-mcp --cli apps compare "ETL Pipeline"        # Auto-compare last 2 matching
uv run spark-mcp --cli apps compare app1 app2             # Compare specific IDs
uv run spark-mcp --cli apps compare app1 app2 --all       # Show all executor metrics

# Interactive configuration
uv run spark-mcp --cli config init --interactive

# Test server connectivity
uv run spark-mcp --cli server test
```

### ğŸ“Š Rich Output Formats
- **Human** (default): Beautiful tables and panels with colors
- **JSON**: Machine-readable for scripting
- **Table**: Simple tabular format

ğŸ“š **[See CLI-README.md](CLI-README.md) for complete CLI documentation**
ğŸ“š **[MCP Tools Reference](docs/mcp-tools.md) (responses + samples)**

---

If you just want to run the MCP server without cloning the repository:

```bash
# Run with uv without installing the module
uvx --from mcp-apache-spark-history-server spark-mcp

# OR run with pip and python. Use of venv is highly encouraged.
python3 -m venv spark-mcp && source spark-mcp/bin/activate
pip install mcp-apache-spark-history-server
python3 -m spark_history_mcp.core.main
# Deactivate venv
deactivate
```


### ğŸ“Š Sample Data
The repository includes real Spark event logs for testing:
- `spark-bcec39f6201b42b9925124595baad260` - âœ… Successful ETL job
- `spark-110be3a8424d4a2789cb88134418217b` - ğŸ”„ Data processing job
- `spark-cc4d115f011443d787f03a71a476a745` - ğŸ“ˆ Multi-stage analytics job

See **[TESTING.md](TESTING.md)** for using them.

### âš™ï¸ Server Configuration
Edit `config.yaml` for your Spark History Server:
```yaml
servers:
  local:
    default: true
    url: "http://your-spark-history-server:18080"
    auth:  # optional
      username: "user"
      password: "pass"
mcp:
  transports:
    - streamable-http # streamable-http or stdio.
  port: "18888"
  debug: true
```

## ğŸ“¸ Screenshots

### ğŸ” Get Spark Application
![Get Application](screenshots/get-application.png)

### âš¡ Job Performance Comparison
![Job Comparison](screenshots/job-compare.png)


## ğŸš€ Major Enhancements Beyond Original Fork

This MCP server has been significantly enhanced with advanced capabilities far beyond the original forked functionality:

### ğŸ“ˆ **Massive Tool Expansion**
- **18 original tools** â†’ **50+ current tools** (3x growth)
- **Comprehensive comparison suite** with 8 advanced tools for cross-application analysis
- **Timeline-based analysis** with intelligent interval merging and executor-focused comparisons

### ğŸ§  **SparkInsight Intelligence Integration**
- **5 AI-powered analysis tools** inspired by SparkInsight for intelligent performance optimization
- **Auto-scaling recommendations** based on workload pattern analysis
- **Data skew detection** with actionable optimization suggestions
- **Failure analysis** with root cause identification
- **Executor utilization tracking** for resource optimization

### ğŸ’¬ **Intelligent Prompts System**
- **16 structured prompts** for systematic Spark analysis
- **Domain expertise** encoded into reusable templates
- **Tailored analysis** for different audiences (technical vs executive)
- **Consistent methodology** across performance investigations

### â° **Advanced Timeline Analysis**
- **Executor timeline comparisons** at both application and stage levels
- **Simplified output focus** removing noise from memory/CPU metrics
- **Intelligent interval merging** for cleaner analysis
- **Resource allocation pattern** tracking over time

### ğŸ¯ **Enhanced Comparative Analysis**
- **Multi-dimensional comparisons**: resources, executors, jobs, stages
- **Comprehensive performance analysis** with stage-level deep dives
- **Significance filtering** to highlight meaningful differences
- **Actionable recommendations** for optimization

Tools marked with ğŸ†• represent major additions beyond the original fork functionality.

## ğŸ› ï¸ Available Tools

> **Note**: These tools are subject to change as we scale and improve the performance of the MCP server.

The MCP server provides **50+ specialized tools** and **16 intelligent prompts** organized by analysis patterns. LLMs can intelligently select and combine these tools and prompts based on user queries:

### ğŸ“Š Application Information
*Basic application metadata and overview*
| ğŸ”§ Tool | ğŸ“ Description |
|---------|----------------|
| `get_application` | ğŸ“Š Get detailed information about a specific Spark application including status, resource usage, duration, and attempt details |
| `list_applications` ğŸ†• | ğŸ“‹ **Enhanced application discovery** - Advanced filtering by status, dates, limits, and flexible name matching (exact, contains, regex patterns) |

### ğŸ”— Job Analysis
*Job-level performance analysis and identification*
| ğŸ”§ Tool | ğŸ“ Description |
|---------|----------------|
| `list_jobs` | ğŸ”— Get a list of all jobs for a Spark application with optional status filtering |
| `list_slowest_jobs` | â±ï¸ Get the N slowest jobs for a Spark application (excludes running jobs by default) |

### âš¡ Stage Analysis
*Stage-level performance deep dive and task metrics*
| ğŸ”§ Tool | ğŸ“ Description |
|---------|----------------|
| `list_stages` | âš¡ Get a list of all stages for a Spark application with optional status filtering and summaries |
| `list_slowest_stages` | ğŸŒ Get the N slowest stages for a Spark application (excludes running stages by default) |
| `get_stage` | ğŸ¯ Get information about a specific stage with optional attempt ID and summary metrics |
| `get_stage_task_summary` | ğŸ“Š Get statistical distributions of task metrics for a specific stage (execution times, memory usage, I/O metrics) |

### ğŸ–¥ï¸ Executor & Resource Analysis
*Resource utilization, executor performance, and allocation tracking*
| ğŸ”§ Tool | ğŸ“ Description |
|---------|----------------|
| `list_executors` | ğŸ–¥ï¸ Get executor information with optional inactive executor inclusion |
| `get_executor` | ğŸ” Get information about a specific executor including resource allocation, task statistics, and performance metrics |
| `get_executor_summary` | ğŸ“ˆ Aggregates metrics across all executors (memory usage, disk usage, task counts, performance metrics) |
| `get_resource_usage_timeline` | ğŸ“… Get chronological view of resource allocation and usage patterns including executor additions/removals |

### âš™ï¸ Configuration & Environment
*Spark configuration, environment variables, and runtime settings*
| ğŸ”§ Tool | ğŸ“ Description |
|---------|----------------|
| `get_environment` | âš™ï¸ Get comprehensive Spark runtime configuration including JVM info, Spark properties, system properties, and classpath |

### ğŸ” SQL & Query Analysis
*SQL performance analysis and execution plan comparison*
| ğŸ”§ Tool | ğŸ“ Description |
|---------|----------------|
| `list_slowest_sql_queries` | ğŸŒ Get the top N slowest SQL queries for an application with detailed execution metrics |
| `compare_sql_execution_plans` | ğŸ” Compare SQL execution plans between two Spark jobs, analyzing logical/physical plans and execution metrics |

### ğŸš¨ Performance & Bottleneck Analysis
*Intelligent bottleneck identification and performance recommendations*
| ğŸ”§ Tool | ğŸ“ Description |
|---------|----------------|
| `get_job_bottlenecks` | ğŸš¨ Identify performance bottlenecks by analyzing stages, tasks, and executors with actionable recommendations |

### ğŸ”„ Comparative Analysis
*Cross-application comparison for regression detection and optimization*

#### ğŸ—ï¸ Application-Level Comparison
| ğŸ”§ Tool | ğŸ“ Description |
|---------|----------------|
| `compare_job_environments` | âš™ï¸ Compare Spark environment configurations between two jobs to identify differences in properties and settings |
| `compare_job_performance` | ğŸ“ˆ Compare performance metrics between two Spark jobs including execution times, resource usage, and task distribution |
| `compare_app_resources` ğŸ†• | ğŸ’° Compare resource allocation and configuration patterns between applications focusing on executor setup and utilization efficiency |
| `compare_app_executors` ğŸ†• | ğŸ–¥ï¸ Compare executor-level performance metrics including memory usage, GC performance, and task completion patterns |
| `compare_app_jobs` ğŸ†• | ğŸ”— Compare job-level performance metrics focusing on job counts, durations, success rates, and parallelism patterns |
| `compare_app_stages_aggregated` ğŸ†• | âš¡ Compare overall stage performance patterns, I/O volumes, and shuffle operations without individual stage details |
| `compare_app_performance` ğŸ†• | ğŸ¯ **Ultimate performance comparison** - Multi-dimensional analysis covering resources, jobs, executors, and stages with intelligent filtering and actionable recommendations |

#### â° Timeline-Based Comparison ğŸ†•
*Advanced timeline analysis with simplified executor-focused output and interval merging*
| ğŸ”§ Tool | ğŸ“ Description |
|---------|----------------|
| `compare_app_executor_timeline` ğŸ†• | ğŸ“… **Advanced timeline comparison** - Analyze executor allocation patterns across entire application lifecycle with intelligent interval merging and noise reduction |
| `compare_stage_executor_timeline` ğŸ†• | ğŸ• **Stage-level timeline analysis** - Compare executor patterns during specific stages with consolidated intervals and simplified executor-focused output |

#### ğŸ¯ Granular Component Comparison
| ğŸ”§ Tool | ğŸ“ Description |
|---------|----------------|
| `compare_stages` ğŸ†• | âš¡ Compare specific stages between two applications focusing on significant performance differences only |

### ğŸ§  SparkInsight Intelligence ğŸ†•
*AI-powered analysis tools inspired by SparkInsight for intelligent performance optimization*
| ğŸ”§ Tool | ğŸ“ Description |
|---------|----------------|
| `analyze_auto_scaling` ğŸ†• | ğŸš€ Analyze workload patterns and provide intelligent auto-scaling recommendations for dynamic allocation |
| `analyze_shuffle_skew` ğŸ†• | ğŸ“Š Detect and analyze data skew in shuffle operations with actionable optimization suggestions |
| `analyze_failed_tasks` ğŸ†• | ğŸš¨ Investigate task failures to identify patterns, problematic executors, and root causes |
| `analyze_executor_utilization` ğŸ†• | ğŸ“ˆ Track executor utilization over time to identify over/under-provisioning and optimization opportunities |
| `get_application_insights` ğŸ†• | ğŸ§  **Comprehensive SparkInsight analysis** - Runs all analyzers to provide complete performance overview and recommendations |

### ğŸ’¬ Intelligent Prompts ğŸ†•
*Reusable templates that guide AI agents in structured Spark analysis*

The MCP server provides **16 intelligent prompts** organized by analysis type. These prompts help AI agents ask better questions and generate more structured, comprehensive analyses:

#### ğŸ“Š Performance Analysis Prompts
| ğŸ’¬ Prompt | ğŸ“ Description |
|-----------|----------------|
| `analyze_slow_application` | ğŸŒ Generate structured analysis framework for slow-running Spark applications with baseline comparison |
| `investigate_stage_bottlenecks` | âš¡ Create detailed prompt for investigating stage-level performance bottlenecks and task-level issues |
| `diagnose_resource_issues` | ğŸ” Generate systematic resource utilization diagnosis across memory, CPU, disk, and network dimensions |
| `compare_job_performance` | ğŸ“ˆ Structure comprehensive performance comparison between two Spark applications |

#### ğŸ› ï¸ Troubleshooting & Debugging Prompts
| ğŸ’¬ Prompt | ğŸ“ Description |
|-----------|----------------|
| `investigate_failures` | ğŸš¨ Systematic investigation framework for application failures, task errors, and reliability issues |
| `examine_memory_issues` | ğŸ§  Detailed memory problem diagnosis including heap, off-heap, GC pressure, and spill analysis |
| `diagnose_shuffle_problems` | ğŸ”„ Comprehensive shuffle operation diagnosis focusing on skew, performance, and data movement issues |
| `identify_configuration_issues` | âš™ï¸ Systematic configuration assessment and optimization opportunity identification |

#### ğŸš€ Optimization Prompts
| ğŸ’¬ Prompt | ğŸ“ Description |
|-----------|----------------|
| `suggest_autoscaling_config` | ğŸ“Š Generate auto-scaling configuration recommendations with cost-performance optimization |
| `optimize_resource_allocation` | ğŸ’¡ Comprehensive resource allocation optimization across executors, memory, and CPU dimensions |
| `improve_query_performance` | ğŸƒ SQL query and data processing performance optimization with execution plan analysis |
| `reduce_data_skew` | âš–ï¸ Comprehensive data skew reduction strategies including preprocessing and runtime solutions |

#### ğŸ“‹ Reporting & Summary Prompts
| ğŸ’¬ Prompt | ğŸ“ Description |
|-----------|----------------|
| `generate_performance_report` | ğŸ“„ Create comprehensive performance reports tailored for different audiences (executive/technical) |
| `create_executive_summary` | ğŸ‘” Generate high-level executive summaries focused on business impact and strategic recommendations |
| `summarize_trends` | ğŸ“ˆ Analyze trends and patterns across multiple Spark applications over time |
| `benchmark_comparison` | ğŸ¯ Compare application performance against internal benchmarks, industry standards, or historical data |

### ğŸ¤– How LLMs Use These Tools & Prompts

**Query Pattern Examples:**

**Basic Analysis:**
- *"Why is my job slow?"* â†’ `get_job_bottlenecks` + `list_slowest_stages` + `get_executor_summary`
- *"Compare today vs yesterday"* â†’ `compare_job_performance` + `compare_job_environments`
- *"What's wrong with stage 5?"* â†’ `get_stage` + `get_stage_task_summary`
- *"Show me resource usage over time"* â†’ `get_resource_usage_timeline` + `get_executor_summary`
- *"Find my slowest SQL queries"* â†’ `list_slowest_sql_queries` + `compare_sql_execution_plans`

**Timeline & Resource Analysis:**
- *"How do executor patterns differ between apps?"* â†’ `compare_app_executor_timeline` (simplified executor-focused comparison)
- *"Compare stage resource allocation over time"* â†’ `compare_stage_executor_timeline` (merged interval analysis)
- *"Analyze resource allocation efficiency"* â†’ `compare_app_resources` + `compare_app_executors`
- *"Track executor scaling differences"* â†’ `compare_app_executor_timeline` + `analyze_auto_scaling`

**Comprehensive Analysis with Prompts:**
- *"Thoroughly analyze my slow application"* â†’ `analyze_slow_application` prompt â†’ structured analysis using multiple tools
- *"Generate a performance report for executives"* â†’ `generate_performance_report` prompt â†’ comprehensive business-focused analysis
- *"Help me optimize auto-scaling configuration"* â†’ `suggest_autoscaling_config` prompt â†’ detailed optimization strategy
- *"Investigate memory issues systematically"* â†’ `examine_memory_issues` prompt â†’ structured memory diagnosis
- *"Create executive summary of app performance"* â†’ `create_executive_summary` prompt â†’ high-level business impact analysis

**Advanced Analysis:**
- *"Analyze my app performance with insights"* â†’ `get_application_insights` (comprehensive SparkInsight analysis)
- *"Why are my tasks failing?"* â†’ `investigate_failures` prompt â†’ systematic failure investigation
- *"Check for data skew issues"* â†’ `reduce_data_skew` prompt â†’ comprehensive skew mitigation strategy
- *"Compare multiple applications over time"* â†’ `summarize_trends` prompt â†’ trend analysis across apps

### ğŸ’¡ How Prompts Enhance AI Analysis

**Intelligent Prompts provide:**
- **Structured Frameworks**: Guide AI agents through systematic analysis approaches
- **Domain Expertise**: Encode Spark performance knowledge into reusable templates
- **Tool Recommendations**: Suggest optimal MCP tool sequences for different scenarios
- **Consistent Analysis**: Ensure comprehensive coverage of key performance areas
- **Tailored Output**: Generate analysis appropriate for different audiences (technical, executive)

**Example: Using the `analyze_slow_application` prompt:**
```
User: "My Spark job app-12345 is running slower than expected"
AI Agent: Uses analyze_slow_application("app-12345") prompt
Result: Structured investigation covering:
  â€¢ Application performance baseline comparison
  â€¢ Bottleneck identification using get_job_bottlenecks
  â€¢ Resource utilization analysis via analyze_executor_utilization
  â€¢ Data skew detection with analyze_shuffle_skew
  â€¢ Configuration review and optimization recommendations
```

## ğŸ†• Recent Improvements: Timeline Comparison Tools

The timeline comparison tools have been enhanced with major improvements for cleaner analysis:

### âœ¨ **Simplified Output Focus**
- **Executor-Only Metrics**: Timeline comparisons now focus exclusively on executor count differences, removing memory and CPU noise
- **Streamlined Data**: Cleaner output makes it easier to identify meaningful resource allocation patterns
- **Reduced Complexity**: Simplified comparison data structure improves readability and analysis speed

### ğŸ”„ **Intelligent Interval Merging**
- **Consecutive Consolidation**: Adjacent time intervals with identical executor counts are automatically merged
- **Duration Tracking**: Merged intervals include `duration_intervals` showing how many original intervals were consolidated
- **Noise Reduction**: Significantly reduces output size while preserving all critical information

### ğŸ“Š **Enhanced Summary Statistics**
- **Dual Metrics**: Shows both `original_intervals` and `merged_intervals` counts for transparency
- **Improved Insights**: Summary statistics calculated on merged data provide more meaningful analysis
- **Better Tracking**: Clear visibility into the effectiveness of interval consolidation

### ğŸ› ï¸ **Affected Tools**
- `compare_stage_executor_timeline` - Stage-level timeline comparison with merged intervals
- `compare_app_executor_timeline` - Application-level timeline comparison with simplified output

These improvements make timeline analysis more focused and actionable while maintaining full backward compatibility.

## ğŸ§  SparkInsight Integration

The MCP server now includes intelligent analysis capabilities inspired by SparkInsight! See the **[SparkInsight Integration Guide](examples/sparkinsight/README.md)** for:

- ğŸš€ **Auto-scaling optimization** recommendations
- ğŸ“Š **Data skew detection** and mitigation strategies
- ğŸš¨ **Failure analysis** with root cause identification
- ğŸ“ˆ **Executor utilization** optimization insights
- ğŸ§  **Comprehensive analysis** combining all insights

## ğŸ“” AWS Integration Guides

If you are an existing AWS user looking to analyze your Spark Applications, we provide detailed setup guides for:

- **[AWS Glue Users](examples/aws/glue/README.md)** - Connect to Glue Spark History Server
- **[Amazon EMR Users](examples/aws/emr/README.md)** - Use EMR Persistent UI for Spark analysis

These guides provide step-by-step instructions for setting up the Spark History Server MCP with your AWS services.

## ğŸš€ Kubernetes Deployment

Deploy using Kubernetes with Helm:

> âš ï¸ **Work in Progress**: We are still testing and will soon publish the container image and Helm registry to GitHub for easy deployment.

```bash
# ğŸ“¦ Deploy with Helm
helm install spark-history-mcp ./deploy/kubernetes/helm/spark-history-mcp/

# ğŸ¯ Production configuration
helm install spark-history-mcp ./deploy/kubernetes/helm/spark-history-mcp/ \
  --set replicaCount=3 \
  --set autoscaling.enabled=true \
  --set monitoring.enabled=true
```

ğŸ“š See [`deploy/kubernetes/helm/`](deploy/kubernetes/helm/) for complete deployment manifests and configuration options.

## ğŸŒ Multi-Spark History Server Setup
Setup multiple Spark history servers in the config.yaml and choose which server you want the LLM to interact with for each query.

```yaml
servers:
  production:
    default: true
    url: "http://prod-spark-history:18080"
    auth:
      username: "user"
      password: "pass"
  staging:
    url: "http://staging-spark-history:18080"
```

ğŸ’ User Query: "Can you get application <app_id> using production server?"

ğŸ¤– AI Tool Request:
```json
{
  "app_id": "<app_id>",
  "server": "production"
}
```
ğŸ¤– AI Tool Response:
```json
{
  "id": "<app_id>>",
  "name": "app_name",
  "coresGranted": null,
  "maxCores": null,
  "coresPerExecutor": null,
  "memoryPerExecutorMB": null,
  "attempts": [
    {
      "attemptId": null,
      "startTime": "2023-09-06T04:44:37.006000Z",
      "endTime": "2023-09-06T04:45:40.431000Z",
      "lastUpdated": "2023-09-06T04:45:42Z",
      "duration": 63425,
      "sparkUser": "spark",
      "appSparkVersion": "3.3.0",
      "completed": true
    }
  ]
}
```

### ğŸ” Environment Variables
```
SHS_MCP_PORT - Port for MCP server (default: 18888)
SHS_MCP_DEBUG - Enable debug mode (default: false)
SHS_MCP_ADDRESS - Address for MCP server (default: localhost)
SHS_MCP_TRANSPORT - MCP transport mode (default: streamable-http)
SHS_SERVERS_*_URL - URL for a specific server
SHS_SERVERS_*_AUTH_USERNAME - Username for a specific server
SHS_SERVERS_*_AUTH_PASSWORD - Password for a specific server
SHS_SERVERS_*_AUTH_TOKEN - Token for a specific server
SHS_SERVERS_*_VERIFY_SSL - Whether to verify SSL for a specific server (true/false)
SHS_SERVERS_*_TIMEOUT - HTTP request timeout in seconds for a specific server (default: 30)
SHS_SERVERS_*_EMR_CLUSTER_ARN - EMR cluster ARN for a specific server
```

## ğŸ¤– AI Agent Integration

### Quick Start Options

| Integration | Transport | Best For |
|-------------|-----------|----------|
| **[Local Testing](TESTING.md)** | HTTP | Development, testing tools |
| **[Claude Desktop](examples/integrations/claude-desktop/)** | STDIO | Interactive analysis |
| **[Amazon Q CLI](examples/integrations/amazon-q-cli/)** | STDIO | Command-line automation |
| **[Kiro](examples/integrations/kiro/)** | HTTP | IDE integration, code-centric analysis |
| **[LangGraph](examples/integrations/langgraph/)** | HTTP | Multi-agent workflows |
| **[Strands Agents](examples/integrations/strands-agents/)** | HTTP | Multi-agent workflows |

## ğŸ¯ Example Use Cases

### ğŸ” Performance Investigation
```
ğŸ¤– AI Query: "Why is my ETL job running slower than usual?"

ğŸ“Š MCP Actions:
âœ… Analyze application metrics
âœ… Compare with historical performance
âœ… Identify bottleneck stages
âœ… Generate optimization recommendations
```

### ğŸš¨ Failure Analysis
```
ğŸ¤– AI Query: "What caused job 42 to fail?"

ğŸ” MCP Actions:
âœ… Examine failed tasks and error messages
âœ… Review executor logs and resource usage
âœ… Identify root cause and suggest fixes
```

### ğŸ“ˆ Comparative Analysis
```
ğŸ¤– AI Query: "Compare today's batch job with yesterday's run"

ğŸ“Š MCP Actions:
âœ… Compare execution times and resource usage
âœ… Identify performance deltas
âœ… Highlight configuration differences
```

## ğŸ¤ Contributing

Check [CONTRIBUTING.md](CONTRIBUTING.md) for full guidelines on contributions

## ğŸ“„ License

Apache License 2.0 - see [LICENSE](LICENSE) file for details.


## ğŸ“ Trademark Notice

*This project is built for use with Apache Sparkâ„¢ History Server. Not affiliated with or endorsed by the Apache Software Foundation.*

---

<div align="center">

**ğŸ”¥ Connect your Spark infrastructure to AI agents**

[ğŸš€ Get Started](#-quick-start) | [ğŸ› ï¸ View Tools](#%EF%B8%8F-available-tools) | [ğŸ§ª Test Now](TESTING.md) | [ğŸ¤ Contribute](#-contributing)

*Built by the community, for the community* ğŸ’™

</div>
